{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell loads the model from the config file and initializes the viewer\n",
    "'''\n",
    "# %matplotlib widget\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from nerfstudio.utils.eval_utils import eval_setup\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from nerfstudio.viewer.viewer import Viewer\n",
    "from nerfstudio.configs.base_config import ViewerConfig\n",
    "import cv2\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "from typing import List,Optional,Literal\n",
    "from nerfstudio.utils import writer\n",
    "import time\n",
    "from threading import Lock\n",
    "config = Path(\"outputs/garfield_plushie/dig/2024-04-08_095834/config.yml\")#with garfield, patch size 14, with denoising, 48->64 dim, 1260\n",
    "\n",
    "# config = Path(\"outputs/table_scan/dig/2024-03-21_123358/config.yml\")#with garfield, patch size 14, with denoising, 48->64 dim\n",
    "\n",
    "# config = Path(\"outputs/nerfgun2/dig/2024-04-05_110412/config.yml\")#with garfield, patch size 14, with denoising, 48->64 dim, res 1260\n",
    "\n",
    "# config = Path(\"outputs/boops_mug/dig/2024-03-20_110937/config.yml\")#with garfield, patch size 14, with denoising, 48->64 dim\n",
    "\n",
    "# config = Path(\"outputs/tissue_scan/dig/2024-03-21_141012/config.yml\")#with garfield, patch size 14, with denoising, 48->64 dim\n",
    "train_config,pipeline,_,_ = eval_setup(config)\n",
    "dino_loader = pipeline.datamanager.dino_dataloader\n",
    "train_config.logging.local_writer.enable = False\n",
    "# We need to set up the writer to track number of rays, otherwise the viewer will not calculate the resolution correctly\n",
    "writer.setup_local_writer(train_config.logging, max_iter=train_config.max_num_iterations)\n",
    "v = Viewer(ViewerConfig(default_composite_depth=False,num_rays_per_chunk=-1),config.parent,pipeline.datamanager.get_datapath(),pipeline,train_lock=Lock())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "from typing import Union\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "model.to('cuda')\n",
    "def get_depth(img: Union[torch.tensor,np.ndarray]):\n",
    "    assert img.shape[2] == 3\n",
    "    if isinstance(img,torch.Tensor):\n",
    "        img = img.cpu().numpy()\n",
    "    image = Image.fromarray(img)\n",
    "\n",
    "    # prepare image for the model\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    inputs['pixel_values'] = inputs['pixel_values'].cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_depth = outputs.predicted_depth\n",
    "\n",
    "    # interpolate to original size\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        predicted_depth.unsqueeze(1),\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    return prediction.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell defines a simple pose optimizer for learning a rigid transform offset given a gaussian model, star pose, and starting view\n",
    "\"\"\"\n",
    "from lerf.dig import DiGModel\n",
    "from lerf.data.utils.dino_dataloader import DinoDataloader\n",
    "from nerfstudio.cameras.cameras import Cameras\n",
    "from copy import deepcopy\n",
    "from torchvision.transforms.functional import resize\n",
    "import torchvision\n",
    "from gsplat._torch_impl import quat_to_rotmat\n",
    "from contextlib import nullcontext\n",
    "# from nerfstudio.model_components.losses import depth_ranking_loss\n",
    "\n",
    "def get_vid_frame(cap,timestamp):\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Calculate the frame number based on the timestamp and fps\n",
    "    frame_number = min(int(timestamp * fps),int(cap.get(cv2.CAP_PROP_FRAME_COUNT)-1))\n",
    "    \n",
    "    # Set the video position to the calculated frame number\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    \n",
    "    # Read the frame\n",
    "    success, frame = cap.read()\n",
    "    # convert BGR to RGB\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    return frame\n",
    "        \n",
    "def quatmul(q0:torch.Tensor,q1:torch.Tensor):\n",
    "    w0, x0, y0, z0 = torch.unbind(q0, dim=-1)\n",
    "    w1, x1, y1, z1 = torch.unbind(q1, dim=-1)\n",
    "    return torch.stack(\n",
    "            [\n",
    "                -x0 * x1 - y0 * y1 - z0 * z1 + w0 * w1,\n",
    "                x0 * w1 + y0 * z1 - z0 * y1 + w0 * x1,\n",
    "                -x0 * z1 + y0 * w1 + z0 * x1 + w0 * y1,\n",
    "                x0 * y1 - y0 * x1 + z0 * w1 + w0 * z1,\n",
    "            ],\n",
    "            dim = -1\n",
    "        )\n",
    "\n",
    "def cauchy_loss(x:torch.Tensor, y:torch.Tensor, scale:float = 1.0):\n",
    "    \"\"\"\n",
    "    Cauchy loss between x and y\n",
    "    \"\"\"\n",
    "    return torch.log(1 + ((x - y) / scale) ** 2).mean()\n",
    "\n",
    "def depth_ranking_loss(rendered_depth, gt_depth):\n",
    "    \"\"\"\n",
    "    Depth ranking loss as described in the SparseNeRF paper\n",
    "    Assumes that the layout of the batch comes from a PairPixelSampler, so that adjacent samples in the gt_depth\n",
    "    and rendered_depth are from pixels with a radius of each other\n",
    "    \"\"\"\n",
    "    m = 1e-4\n",
    "    if rendered_depth.shape[0] % 2 != 0:\n",
    "        # chop off one index\n",
    "        rendered_depth = rendered_depth[:-1, :]\n",
    "        gt_depth = gt_depth[:-1, :]\n",
    "    dpt_diff = gt_depth[::2, :] - gt_depth[1::2, :]\n",
    "    out_diff = rendered_depth[::2, :] - rendered_depth[1::2, :] + m\n",
    "    differing_signs = torch.sign(dpt_diff) != torch.sign(out_diff)\n",
    "    loss = (out_diff[differing_signs] * torch.sign(out_diff[differing_signs]))\n",
    "    med = loss.median()\n",
    "    return loss[loss < med].mean()\n",
    "\n",
    "@torch.no_grad\n",
    "def get_scale_and_shift(x:torch.Tensor):\n",
    "    shift = x.median()\n",
    "    devs = (x - shift).abs()#median deviation is the scale\n",
    "    return devs.median(),shift \n",
    "\n",
    "class RigidGroupOptimizer:\n",
    "    use_depth: bool = True\n",
    "    def __init__(self, dig_model: DiGModel, dino_loader: DinoDataloader, init_c2o: Cameras, group_masks: List[torch.Tensor],render_lock = nullcontext()):\n",
    "        \"\"\"\n",
    "        This one takes in a list of gaussian ID masks to optimize local poses for\n",
    "        Each rigid group can be optimized independently, with no skeletal constraints\n",
    "        \"\"\"\n",
    "        self.dig_model = dig_model\n",
    "        #detach all the params to avoid retain_graph issue\n",
    "        self.dig_model.gauss_params['means'] = self.dig_model.gauss_params['means'].detach()\n",
    "        self.dig_model.gauss_params['quats'] = self.dig_model.gauss_params['quats'].detach()\n",
    "        self.dino_loader = dino_loader\n",
    "        self.group_masks = group_masks\n",
    "        self.init_c2o = deepcopy(init_c2o).to('cuda')\n",
    "        #store a 7-vec of trans, rotation for each group\n",
    "        self.pose_deltas = torch.zeros(len(group_masks),7,dtype=torch.float32,device='cuda')\n",
    "        self.pose_deltas[:,3:] = torch.tensor([1,0,0,0],dtype=torch.float32,device='cuda')\n",
    "        self.pose_deltas = torch.nn.Parameter(self.pose_deltas)\n",
    "        lr = .01\n",
    "        self.optimizer = torch.optim.Adamax([self.pose_deltas],lr=lr)\n",
    "        self.init_means = dig_model.gauss_params['means'].detach().clone()\n",
    "        self.init_quats = dig_model.gauss_params['quats'].detach().clone()\n",
    "        self.blur = torchvision.transforms.GaussianBlur(kernel_size=[21,21]).cuda()\n",
    "        self.keyframes = []\n",
    "        # lock to prevent blocking the render thread if provided\n",
    "        self.render_lock = render_lock\n",
    "\n",
    "    def step(self, niter = 1, use_depth = True, use_rgb = False):\n",
    "        self.dig_model.eval()\n",
    "        for i in range(niter):\n",
    "            self.optimizer.zero_grad()\n",
    "            with self.render_lock:\n",
    "                self.dig_model.eval()\n",
    "                self.apply_to_model(self.pose_deltas)\n",
    "                dig_outputs = self.dig_model.get_outputs(self.init_c2o)\n",
    "            if 'dino' not in dig_outputs:\n",
    "                self.reset_transforms()\n",
    "                raise RuntimeError(\"Lost tracking\")\n",
    "            dino_feats = self.blur(dig_outputs[\"dino\"].permute(2,0,1)).permute(1,2,0).contiguous()\n",
    "            pix_loss = (self.frame_pca_feats - dino_feats)\n",
    "            # THIS IS BAD WE NEED TO FIX THIS (because resizing makes the image very slightly misaligned)\n",
    "            loss = pix_loss.abs().mean()\n",
    "            if use_depth and self.use_depth:\n",
    "                object_mask = dig_outputs['accumulation']>.9\n",
    "                disparity = 1.0 / dig_outputs['depth']\n",
    "                N = 10000\n",
    "                valid_ids = torch.where(object_mask)\n",
    "                rand_samples = torch.randint(0,valid_ids[0].shape[0],(N,))\n",
    "                rand_samples = (valid_ids[0][rand_samples],valid_ids[1][rand_samples])\n",
    "                rend_samples = disparity[rand_samples]\n",
    "                mono_samples = self.frame_depth[rand_samples]\n",
    "                rank_loss = depth_ranking_loss(rend_samples,mono_samples)\n",
    "                '''Things to trry\n",
    "                ignore rim by eroding mask\n",
    "                try huber, cauchy, or other robust loss\n",
    "                maybe ignore upper quantile of loss or something\n",
    "                '''\n",
    "                loss = loss + .1*rank_loss\n",
    "            if use_rgb:\n",
    "                loss = loss + .1*(dig_outputs['rgb']-self.rgb_frame).abs().mean()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return dig_outputs\n",
    "    \n",
    "    def apply_to_model(self, pose_deltas):\n",
    "        \"\"\"\n",
    "        Takes the current pose_deltas and applies them to each of the group masks\n",
    "        \"\"\"\n",
    "        self.reset_transforms()\n",
    "        quat_deltas = torch.empty((self.dig_model.num_points,4),dtype=torch.float32,device='cuda', requires_grad=True)\n",
    "        Hs = torch.empty((self.dig_model.num_points,3,4),dtype=torch.float32,device='cuda', requires_grad=True)\n",
    "        for i,mask in enumerate(self.group_masks):\n",
    "            ps = pose_deltas[i:i+1,3:]/pose_deltas[i:i+1,3:].norm(dim=1,keepdim=True)\n",
    "            quat_deltas = torch.where(mask[...,None],ps,quat_deltas)\n",
    "            H = torch.cat([quat_to_rotmat(pose_deltas[i:i+1,3:]),pose_deltas[i:i+1,:3].T.unsqueeze(0)],dim=2)\n",
    "            Hs = torch.where(mask[...,None,None],H,Hs)\n",
    "        with torch.no_grad():\n",
    "            self.dig_model.gauss_params['quats'] = quatmul(quat_deltas,self.dig_model.gauss_params['quats'])\n",
    "        self.dig_model.gauss_params['means'] = Hs[:,:3,3] + torch.bmm(Hs[:,:3,:3],self.dig_model.gauss_params['means'][...,None]).squeeze()\n",
    "\n",
    "    def register_keyframe(self):\n",
    "        \"\"\"\n",
    "        Saves the current pose_deltas as a keyframe\n",
    "        \"\"\"\n",
    "        self.keyframes.append(self.pose_deltas.detach().clone())\n",
    "\n",
    "    def apply_keyframe(self,i):\n",
    "        \"\"\"\n",
    "        Applies the ith keyframe to the pose_deltas\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.apply_to_model(self.keyframes[i])\n",
    "\n",
    "\n",
    "\n",
    "    def reset_transforms(self):\n",
    "        with torch.no_grad():\n",
    "            self.dig_model.gauss_params['means'] = self.init_means.clone()\n",
    "            self.dig_model.gauss_params['quats'] = self.init_quats.clone()\n",
    "\n",
    "    def set_frame(self, rgb_frame: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Sets the rgb_frame to optimize the pose for\n",
    "        rgb_frame: HxWxC tensor image\n",
    "        init_c2o: initial camera to object transform (given whatever coordinates the self.dig_model is in)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.rgb_frame = resize(rgb_frame.permute(2,0,1), (self.init_c2o.height,self.init_c2o.width)).permute(1,2,0).contiguous()\n",
    "            self.frame_pca_feats = self.dino_loader.get_pca_feats(rgb_frame.permute(2,0,1).unsqueeze(0),keep_cuda=True).squeeze()\n",
    "            self.frame_pca_feats = resize(self.frame_pca_feats.permute(2,0,1), (self.init_c2o.height,self.init_c2o.width)).permute(1,2,0).contiguous()\n",
    "            if self.use_depth:\n",
    "                depth = get_depth((rgb_frame*255).to(torch.uint8))\n",
    "                self.frame_depth = resize(depth.unsqueeze(0), (self.init_c2o.height,self.init_c2o.width)).squeeze().unsqueeze(-1)\n",
    "\n",
    "MATCH_RESOLUTION = 500\n",
    "train_cam_pose,data = pipeline.datamanager.next_train(0)\n",
    "view_cam_pose = pipeline.viewer_control.get_camera(200,None,0)\n",
    "train_cam_pose.camera_to_worlds = view_cam_pose.camera_to_worlds\n",
    "train_cam_pose.rescale_output_resolution(MATCH_RESOLUTION/max(train_cam_pose.width,train_cam_pose.height))\n",
    "outputs = pipeline.model.get_outputs_for_camera(train_cam_pose)\n",
    "if pipeline.cluster_labels is not None:\n",
    "    labels = pipeline.cluster_labels.int()\n",
    "    group_masks = [(cid == labels).cuda() for cid in range(labels.max() + 1)]\n",
    "else:\n",
    "    group_masks = [torch.ones(pipeline.model.num_points).bool().cuda()]\n",
    "optimizer = RigidGroupOptimizer(pipeline.model,dino_loader,train_cam_pose,group_masks,render_lock = v.train_lock)\n",
    "rgb_renders = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import moviepy.editor as mpy\n",
    "import tqdm\n",
    "video_path = Path(\"garfield_move.mp4\")\n",
    "assert video_path.exists()\n",
    "motion_clip = cv2.VideoCapture(str(video_path.absolute()))\n",
    "start=4.5\n",
    "end=8\n",
    "fps = 60\n",
    "frame = get_vid_frame(motion_clip,start)\n",
    "target_frame_rgb = ToTensor()(Image.fromarray(frame)).permute(1,2,0).cuda()\n",
    "_,axs = plt.subplots(1,2,figsize=(10,3))\n",
    "axs[0].imshow(outputs[\"rgb\"].detach().cpu().numpy())\n",
    "axs[1].imshow(target_frame_rgb.cpu().numpy())\n",
    "plt.show()\n",
    "optimizer.set_frame(target_frame_rgb)\n",
    "\n",
    "# depth = get_depth(frame)\n",
    "# plt.imshow(depth.cpu().numpy())\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerfstudio.utils.colormaps import apply_depth_colormap\n",
    "try:\n",
    "    animate_button.remove()\n",
    "    frame_slider.remove()\n",
    "    reset_button.remove()\n",
    "except:\n",
    "    pass\n",
    "if len(rgb_renders)==0:\n",
    "    for i in tqdm.tqdm(range(10)):\n",
    "        target_vis_frame = resize(target_frame_rgb.permute(2,0,1),(outputs[\"rgb\"].shape[0],outputs[\"rgb\"].shape[1])).permute(1,2,0)\n",
    "        #composite the outputs['rgb'] on top of target_vis frame\n",
    "        target_vis_frame = target_vis_frame*0.5 + outputs[\"rgb\"]*0.5\n",
    "        vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1)\n",
    "        rgb_renders.append(vis_frame.detach().cpu().numpy()*255)\n",
    "        outputs = optimizer.step(20,use_depth=False)\n",
    "for t in tqdm.tqdm(np.linspace(start,end,int((end-start)*fps))):\n",
    "    frame = get_vid_frame(motion_clip,t)\n",
    "    target_frame_rgb = ToTensor()(Image.fromarray(frame)).permute(1,2,0).cuda()\n",
    "    optimizer.set_frame(target_frame_rgb)\n",
    "    optimizer.step(30)\n",
    "    outputs = optimizer.step(10,use_rgb=True)\n",
    "    optimizer.register_keyframe()\n",
    "    v._trigger_rerender()\n",
    "    target_vis_frame = resize(target_frame_rgb.permute(2,0,1),(outputs[\"rgb\"].shape[0],outputs[\"rgb\"].shape[1])).permute(1,2,0)\n",
    "    # composite the outputs['rgb'] on top of target_vis frame\n",
    "    target_vis_frame = target_vis_frame*0.5 + outputs[\"rgb\"]*0.5\n",
    "    vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1)\n",
    "    rgb_renders.append(vis_frame.detach().cpu().numpy()*255)\n",
    "#save as an mp4\n",
    "out_clip = mpy.ImageSequenceClip(rgb_renders, fps=fps)\n",
    "out_clip.write_videofile(\"optimizer_out_lowerrgbweight.mp4\", fps=fps,)\n",
    "\n",
    "# Populate some viewer elements to visualize the animation\n",
    "animate_button = v.viser_server.add_gui_button(\"Play Animation\")\n",
    "frame_slider = v.viser_server.add_gui_slider(\"Frame\",0,len(optimizer.keyframes)-1,1,0)\n",
    "reset_button = v.viser_server.add_gui_button(\"Reset Transforms\")\n",
    "\n",
    "@animate_button.on_click\n",
    "def play_animation(_):\n",
    "    for i in range(len(optimizer.keyframes)):\n",
    "        optimizer.apply_keyframe(i)\n",
    "        v._trigger_rerender()\n",
    "        time.sleep(1/60)\n",
    "@frame_slider.on_update\n",
    "def apply_keyframe(_):\n",
    "    optimizer.apply_keyframe(frame_slider.value)\n",
    "    v._trigger_rerender()\n",
    "@reset_button.on_click\n",
    "def reset_transforms(_):\n",
    "    optimizer.reset_transforms()\n",
    "    v._trigger_rerender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "pipeline.model.eval()\n",
    "outputs = pipeline.model.get_outputs_for_camera(train_cam_pose)\n",
    "mask = outputs['accumulation']>.9\n",
    "disparity = 1.0/outputs['depth']\n",
    "disparity_mono = optimizer.frame_depth\n",
    "scale_rend,shift_rend = get_scale_and_shift(disparity[mask])\n",
    "scale_mono,shift_mono = get_scale_and_shift(disparity_mono[mask])\n",
    "_,axs = plt.subplots(1,3,figsize=(18,4))\n",
    "rend_norm = (disparity - shift_rend)/scale_rend\n",
    "gt_norm = (disparity_mono - shift_mono)/scale_mono\n",
    "gt_norm[~mask] = 0\n",
    "axs[0].imshow(gt_norm.cpu().numpy())\n",
    "axs[1].imshow(rend_norm.cpu().numpy())\n",
    "# loss = (gt_norm - rend_norm).abs()\n",
    "# losstrunc = loss[mask].quantile(.8)\n",
    "# loss[loss > losstrunc] = 0\n",
    "# loss[~mask] = 0\n",
    "# axs[2].imshow(loss.cpu().numpy())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "please",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
