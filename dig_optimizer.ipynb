{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell loads the model from the config file and initializes the viewer\n",
    "'''\n",
    "%matplotlib widget\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from nerfstudio.utils.eval_utils import eval_setup\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from nerfstudio.viewer.viewer import Viewer\n",
    "from nerfstudio.configs.base_config import ViewerConfig\n",
    "import cv2\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "from typing import List\n",
    "config = Path(\"outputs/garfield_plushie/dig/2024-03-20_114231/config.yml\")#with garfield, patch size 14, with denoising, 48->64 dim\n",
    "\n",
    "# config = Path(\"outputs/nerfgun2/dig/2024-03-20_113021/config.yml\")#with garfield, patch size 14, with denoising, 48->64 dim\n",
    "\n",
    "# config = Path(\"outputs/boops_mug/dig/2024-03-20_110937/config.yml\")#with garfield, patch size 14, with denoising, 48->64 dim\n",
    "_,pipeline,_,_ = eval_setup(config)\n",
    "pipeline.eval()\n",
    "dino_loader = pipeline.datamanager.dino_dataloader\n",
    "v = Viewer(ViewerConfig(default_composite_depth=False),config.parent,pipeline.datamanager.get_datapath(),pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell defines a simple pose optimizer for learning a rigid transform offset given a gaussian model, star pose, and starting view\n",
    "\"\"\"\n",
    "from lerf.dig import DiGModel\n",
    "from lerf.data.utils.dino_dataloader import DinoDataloader\n",
    "from nerfstudio.cameras.cameras import Cameras\n",
    "from nerfstudio.cameras.camera_optimizers import CameraOptimizerConfig, CameraOptimizer\n",
    "from copy import deepcopy\n",
    "from torchvision.transforms.functional import resize\n",
    "import torchvision\n",
    "from nerfstudio.cameras.lie_groups import exp_map_SE3\n",
    "def get_vid_frame(cap,timestamp):\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Calculate the frame number based on the timestamp and fps\n",
    "    frame_number = min(int(timestamp * fps),int(cap.get(cv2.CAP_PROP_FRAME_COUNT)-1))\n",
    "    \n",
    "    # Set the video position to the calculated frame number\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    \n",
    "    # Read the frame\n",
    "    success, frame = cap.read()\n",
    "    # convert BGR to RGB\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    return frame\n",
    "        \n",
    "class Cam2ObjOptimizer:\n",
    "    def __init__(self, dig_model:DiGModel, dino_loader: DinoDataloader, init_c2o: Cameras):\n",
    "        self.dig_model = dig_model\n",
    "        self.cam_optimizer = CameraOptimizer(CameraOptimizerConfig(mode = \"SO3xR3\", trans_l2_penalty = 0.0, rot_l2_penalty = 0.0),1,'cuda')\n",
    "        self.dino_loader = dino_loader\n",
    "        self.init_c2o = deepcopy(init_c2o).to('cuda')\n",
    "        self.dig_model.eval()\n",
    "        self.optimizer = torch.optim.Adam(list(self.cam_optimizer.parameters()),lr=0.02)\n",
    "        self.blur = torchvision.transforms.GaussianBlur(kernel_size=[13,13]).cuda()\n",
    "\n",
    "    def set_frame(self, rgb_frame:torch.Tensor, ):\n",
    "        \"\"\"\n",
    "        Sets the rgb_frame to optimize the pose for\n",
    "        rgb_frame: HxWxC tensor image\n",
    "        init_c2o: initial camera to object transform (given whatever coordinates the self.dig_model is in)\n",
    "        \"\"\"\n",
    "        self.rgb_frame = rgb_frame\n",
    "        self.frame_pca_feats = self.dino_loader.get_pca_feats(rgb_frame.permute(2,0,1).unsqueeze(0)).cuda().squeeze()\n",
    "\n",
    "    def step(self, niters = 1):\n",
    "        for i in range(niters):\n",
    "            self.optimizer.zero_grad()\n",
    "            c2o = deepcopy(self.init_c2o)\n",
    "            c2o.camera_to_worlds.requires_grad = True\n",
    "            c2o.metadata = {'cam_idx':0}\n",
    "            self.cam_optimizer.apply_to_camera(c2o)\n",
    "            dig_outputs = self.dig_model.get_outputs(c2o)\n",
    "            dino_feats = self.blur(dig_outputs[\"dino\"].permute(2,0,1)).permute(1,2,0)\n",
    "            # THIS IS BAD WE NEED TO FIX THIS\n",
    "            frame_feats = resize(self.frame_pca_feats.permute(2,0,1), (dino_feats.shape[0],dino_feats.shape[1])).permute(1,2,0).contiguous()\n",
    "            pix_loss = (frame_feats - dino_feats)\n",
    "            loss = pix_loss.norm()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return dig_outputs\n",
    "from gsplat._torch_impl import quat_to_rotmat\n",
    "from scipy.spatial.transform import Rotation as Rot\n",
    "from typing import Literal\n",
    "class RigidGroupOptimizer():\n",
    "    rot_type: Literal['quat','SE3'] = 'quat'\n",
    "    loss_type: Literal['dino','rgb','both'] = 'both'\n",
    "    def __init__(self, dig_model: DiGModel, dino_loader: DinoDataloader, init_c2o: Cameras, group_masks: List[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        This one takes in a list of gaussian ID masks to optimize local poses for\n",
    "        Each rigid group can be optimized independently, with no skeletal constraints\n",
    "        \"\"\"\n",
    "        self.dig_model = dig_model\n",
    "        #detach all the params to avoid retain_graph issue\n",
    "        self.dig_model.gauss_params['means'] = self.dig_model.gauss_params['means'].detach()\n",
    "        self.dig_model.gauss_params['quats'] = self.dig_model.gauss_params['quats'].detach()\n",
    "        self.dino_loader = dino_loader\n",
    "        self.group_masks = group_masks\n",
    "        self.init_c2o = deepcopy(init_c2o).to('cuda')\n",
    "        #store a 6-vec of trans, rotation for each group\n",
    "        if self.rot_type == 'SE3':\n",
    "            self.pose_deltas = torch.nn.Parameter(torch.zeros(len(group_masks),6,dtype=torch.float32,device='cuda'))\n",
    "        elif self.rot_type == 'quat':\n",
    "            self.pose_deltas = torch.zeros(len(group_masks),7,dtype=torch.float32,device='cuda')\n",
    "            self.pose_deltas[:,3:] = torch.tensor([1,0,0,0],dtype=torch.float32,device='cuda')\n",
    "            self.pose_deltas = torch.nn.Parameter(self.pose_deltas)\n",
    "        lr = .03 if self.loss_type == 'dino' else .01\n",
    "        self.optimizer = torch.optim.Adam([self.pose_deltas],lr=lr)\n",
    "        self.init_means = dig_model.gauss_params['means'].detach().clone()\n",
    "        self.init_quats = dig_model.gauss_params['quats'].detach().clone()\n",
    "        self.blur = torchvision.transforms.GaussianBlur(kernel_size=[5,5]).cuda()\n",
    "\n",
    "    def step(self, niter = 1):\n",
    "        self.dig_model.eval()\n",
    "        for i in range(niter):\n",
    "            self.optimizer.zero_grad()\n",
    "            self.apply_to_model()\n",
    "            dig_outputs = self.dig_model.get_outputs(self.init_c2o)\n",
    "            if 'dino' not in dig_outputs:\n",
    "                self.reset_transforms()\n",
    "                raise RuntimeError(\"Lost tracking\")\n",
    "            if self.loss_type == 'dino':\n",
    "                dino_feats = self.blur(dig_outputs[\"dino\"].permute(2,0,1)).permute(1,2,0).contiguous()\n",
    "                # THIS IS BAD WE NEED TO FIX THIS\n",
    "                pix_loss = (self.frame_pca_feats - dino_feats)\n",
    "                loss = pix_loss.abs().mean()\n",
    "            elif self.loss_type == 'rgb':\n",
    "                rgb_out = dig_outputs[\"rgb\"]\n",
    "                pix_loss = (self.rgb_frame - rgb_out)\n",
    "                loss = pix_loss.abs().mean()\n",
    "            elif self.loss_type == 'both':\n",
    "                rgb_out = dig_outputs[\"rgb\"]\n",
    "                rgb_loss = (self.rgb_frame - rgb_out)\n",
    "                dino_feats = self.blur(dig_outputs[\"dino\"].permute(2,0,1)).permute(1,2,0).contiguous()\n",
    "                # THIS IS BAD WE NEED TO FIX THIS\n",
    "                dino_loss = (self.frame_pca_feats - dino_feats)\n",
    "                loss = (rgb_loss.abs().mean() + dino_loss.abs().mean())/2\n",
    "            loss.backward()\n",
    "            #weight the grads for rotation higher\n",
    "            self.optimizer.step()\n",
    "            self.reset_transforms()\n",
    "        return dig_outputs\n",
    "    \n",
    "    def apply_to_model(self):\n",
    "        \"\"\"\n",
    "        Takes the current pose_deltas and applies them to each of the group masks\n",
    "        \"\"\"\n",
    "        transforms = self.get_3x4s()\n",
    "        self.reset_transforms()\n",
    "        for i,mask in enumerate(self.group_masks):\n",
    "            H = transforms[i]\n",
    "            self.apply_H_to_group(mask, H)\n",
    "\n",
    "    def reset_transforms(self):\n",
    "        with torch.no_grad():\n",
    "            self.dig_model.gauss_params['means'] = self.init_means.clone()\n",
    "            self.dig_model.gauss_params['quats'] = self.init_quats.clone()\n",
    "\n",
    "    def get_3x4s(self):\n",
    "        \"\"\"\n",
    "        Returns a list of 3x4 transforms for each group\n",
    "        \"\"\"\n",
    "        if self.rot_type == 'quat':\n",
    "            Hs = torch.zeros(len(self.group_masks),3,4,dtype=torch.float32,device='cuda')\n",
    "            for i in range(len(self.group_masks)):\n",
    "                Hs[i,:3,3] = self.pose_deltas[i,:3]\n",
    "                Hs[i,:3,:3] = quat_to_rotmat(self.pose_deltas[i,3:])\n",
    "            return Hs\n",
    "        elif self.rot_type == 'SE3':\n",
    "            return exp_map_SE3(self.pose_deltas)\n",
    "    \n",
    "    def apply_H_to_group(self, group_mask:torch.Tensor, H:torch.Tensor):\n",
    "        \"\"\"\n",
    "        Applies the 4x4 transform H to the gaussians in the group mask\n",
    "        \"\"\"\n",
    "        # apply H to the quats\n",
    "        with torch.no_grad():\n",
    "            all_Rs = quat_to_rotmat(self.dig_model.gauss_params['quats'])\n",
    "            all_Rs = torch.matmul(H[:3,:3],all_Rs)\n",
    "            self.dig_model.gauss_params['quats'][group_mask] = torch.tensor(Rot.from_matrix(all_Rs[group_mask].cpu().detach().numpy()).as_quat()).float().cuda()[:, [3, 0, 1, 2]]\n",
    "        # First apply H to the means\n",
    "        muled_means = H[:3,3] + torch.matmul(H[:3,:3],self.dig_model.gauss_params['means'].T).T\n",
    "        self.dig_model.gauss_params['means'] = torch.where(group_mask[...,None],muled_means,self.dig_model.gauss_params['means'])[:,:3]\n",
    "        \n",
    "    def set_frame(self, rgb_frame:torch.Tensor):\n",
    "        \"\"\"\n",
    "        Sets the rgb_frame to optimize the pose for\n",
    "        rgb_frame: HxWxC tensor image\n",
    "        init_c2o: initial camera to object transform (given whatever coordinates the self.dig_model is in)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.rgb_frame = resize(rgb_frame.permute(2,0,1), (self.init_c2o.height,self.init_c2o.width)).permute(1,2,0).contiguous()\n",
    "            self.frame_pca_feats = self.dino_loader.get_pca_feats(rgb_frame.permute(2,0,1).unsqueeze(0),keep_cuda=True).squeeze()\n",
    "            self.frame_pca_feats = resize(self.frame_pca_feats.permute(2,0,1), (self.init_c2o.height,self.init_c2o.width)).permute(1,2,0).contiguous()\n",
    "MATCH_RESOLUTION = 250\n",
    "train_cam_pose,data = pipeline.datamanager.next_train(0)\n",
    "view_cam_pose = pipeline.viewer_control.get_camera(200,None,0)\n",
    "train_cam_pose.camera_to_worlds = view_cam_pose.camera_to_worlds\n",
    "train_cam_pose.rescale_output_resolution(MATCH_RESOLUTION/max(train_cam_pose.width,train_cam_pose.height))\n",
    "outputs = pipeline.model.get_outputs_for_camera(train_cam_pose)\n",
    "plt.imshow(outputs[\"rgb\"].cpu().numpy())\n",
    "plt.show()\n",
    "halfmask = torch.ones(pipeline.model.means.shape[0],dtype=torch.bool,device='cuda')\n",
    "optimizer = RigidGroupOptimizer(pipeline.model,dino_loader,train_cam_pose,[halfmask])\n",
    "rgb_renders = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mpy\n",
    "import tqdm\n",
    "video_path = Path(\"garfield_move.mp4\")\n",
    "assert video_path.exists()\n",
    "motion_clip = cv2.VideoCapture(str(video_path.absolute()))\n",
    "start=4\n",
    "end=8\n",
    "fps = 30\n",
    "frame = get_vid_frame(motion_clip,start)\n",
    "target_frame_rgb = ToTensor()(Image.fromarray(frame)).permute(1,2,0).cuda()\n",
    "optimizer.set_frame(target_frame_rgb)\n",
    "try:\n",
    "    for i in tqdm.tqdm(range(10)):\n",
    "        target_vis_frame = resize(target_frame_rgb.permute(2,0,1),(outputs[\"rgb\"].shape[0],outputs[\"rgb\"].shape[1])).permute(1,2,0)\n",
    "        #composite the outputs['rgb'] on top of target_vis frame\n",
    "        target_vis_frame = target_vis_frame*0.5 + outputs[\"rgb\"]*0.5\n",
    "        vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1)\n",
    "        rgb_renders.append(vis_frame.detach().cpu().numpy()*255)\n",
    "        outputs = optimizer.step(15)\n",
    "    for t in tqdm.tqdm(np.linspace(start,end,int((end-start)*fps))):\n",
    "        frame = get_vid_frame(motion_clip,t)\n",
    "        target_frame_rgb = ToTensor()(Image.fromarray(frame)).permute(1,2,0).cuda()\n",
    "        optimizer.set_frame(target_frame_rgb)\n",
    "        outputs = optimizer.step(10)\n",
    "        target_vis_frame = resize(target_frame_rgb.permute(2,0,1),(outputs[\"rgb\"].shape[0],outputs[\"rgb\"].shape[1])).permute(1,2,0)\n",
    "        #composite the outputs['rgb'] on top of target_vis frame\n",
    "        target_vis_frame = target_vis_frame*0.5 + outputs[\"rgb\"]*0.5\n",
    "        vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1)\n",
    "        rgb_renders.append(vis_frame.detach().cpu().numpy()*255)\n",
    "except RuntimeError:\n",
    "    print(\"Lost tracking\")\n",
    "#save as an mp4\n",
    "out_clip = mpy.ImageSequenceClip(rgb_renders, fps=fps)\n",
    "out_clip.write_videofile(\"both_garfield.mp4\", fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "please",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
